# üë∂ Baby Language Model - START HERE

## ‚úÖ Your Questions Answered

### 1. Is it using RNN?
**YES! It uses LSTM (Long Short-Term Memory), which is a type of RNN.**

```
Model Architecture:
Input ‚Üí Embedding ‚Üí LSTM (RNN) ‚Üí LSTM (RNN) ‚Üí Dense ‚Üí Output
                      ‚Üë              ‚Üë
                   RNN Layer 1   RNN Layer 2
```

### 2. How to run it in main?
**Super easy! Choose ANY method:**

```bash
# Method 1: Simplest
python baby_language_model.py

# Method 2: Quick test
python simple_run.py

# Method 3: Full examples
python run_baby_model.py
```

---

## üéØ 3 Second Start

```bash
pip install tensorflow numpy
python simple_run.py
```

**Done!** Your baby AI will train and talk! üéâ

---

## üìÅ What You Have

```
baby_language_model.py        ‚Üê Main model (RNN/LSTM based) ‚≠ê
simple_run.py                 ‚Üê Easiest way to run ‚≠ê‚≠ê‚≠ê
run_baby_model.py             ‚Üê All examples
baby_model_colab.py           ‚Üê Google Colab version
baby_transformer_model.py     ‚Üê Advanced transformer version

QUICK_START.md                ‚Üê Read this first! ‚≠ê‚≠ê‚≠ê
ARCHITECTURE_EXPLAINED.md     ‚Üê Understand the RNN ‚≠ê
BABY_MODEL_GUIDE.md           ‚Üê Complete guide
START_HERE.md                 ‚Üê You are here!
```

---

## üöÄ Quickest Way to Run

### Option A: One-Liner
```bash
python -c "from baby_language_model import main; main()"
```

### Option B: Simple Script
```bash
python simple_run.py
```

### Option C: Step-by-step
```python
from baby_language_model import BabyLanguageModel, BABY_DATASET

baby = BabyLanguageModel(BABY_DATASET)
baby.build_model()      # Builds the RNN
baby.train(epochs=150)  # Trains the RNN
print(baby.generate_text("me want", length=50))
```

---

## üß† RNN Architecture Visual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT: "me want"                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Embedding Layer (Convert to vectors)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîÑ LSTM LAYER 1 (RNN - 128 units)          ‚îÇ
‚îÇ     ‚Ä¢ Processes character by character       ‚îÇ
‚îÇ     ‚Ä¢ Remembers previous context            ‚îÇ
‚îÇ     ‚Ä¢ return_sequences=True                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dropout (Prevent overfitting)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîÑ LSTM LAYER 2 (RNN - 128 units)          ‚îÇ
‚îÇ     ‚Ä¢ Further pattern learning              ‚îÇ
‚îÇ     ‚Ä¢ Uses memory from layer 1              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dropout (Prevent overfitting)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dense Layer (Predict next character)       ‚îÇ
‚îÇ  Softmax: [c: 40%, o: 20%, m: 15%, ...]    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OUTPUT: "me want cookie"                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíª Running in Different Environments

### Local Machine
```bash
cd /Users/soroushebadi/Desktop/Projects/ML_learning
pip install tensorflow numpy
python simple_run.py
```

### Google Colab
```python
# Cell 1
!pip install tensorflow numpy

# Cell 2
# Paste entire baby_model_colab.py content here

# Cell 3
# Run and watch it train!
```

### Python Shell
```bash
python
```
```python
>>> from baby_language_model import main
>>> baby = main()
```

### Jupyter Notebook
```python
# Cell 1
!pip install tensorflow numpy

# Cell 2
from baby_language_model import BabyLanguageModel, BABY_DATASET

# Cell 3
baby = BabyLanguageModel(BABY_DATASET)
baby.build_model()
baby.train(epochs=150)

# Cell 4
baby.generate_text("me want", length=50)
```

---

## üéÆ Quick Examples

### Example 1: Generate Baby Talk
```python
from baby_language_model import BabyLanguageModel, BABY_DATASET

baby = BabyLanguageModel(BABY_DATASET)
baby.build_model()
baby.train(epochs=100)

# Generate!
print(baby.generate_text("me want", length=50))
print(baby.generate_text("me love", length=50))
print(baby.generate_text("mommy", length=50))
```

### Example 2: Temperature Comparison
```python
prompt = "me want"

print("Safe (0.5):", baby.generate_text(prompt, temperature=0.5))
print("Normal (1.0):", baby.generate_text(prompt, temperature=1.0))
print("Creative (1.5):", baby.generate_text(prompt, temperature=1.5))
```

### Example 3: Save and Load
```python
# Train once
baby.train(epochs=200)
baby.save_model('my_baby.keras')

# Use forever
baby.load_model('my_baby.keras')
baby.generate_text("me want", length=50)
```

---

## üìä What to Expect

### Training Output (2-3 minutes)
```
Epoch 1/150
32/32 [======] - 2s - loss: 2.845 - accuracy: 0.234
Epoch 50/150
32/32 [======] - 1s - loss: 0.812 - accuracy: 0.756
Epoch 150/150
32/32 [======] - 1s - loss: 0.234 - accuracy: 0.932
```

### Generated Output
```
Seed: "me want"
Output: "me want cookie me love mommy me play with toy"

Seed: "me love"
Output: "me love daddy me want hug me happy"

Seed: "me see"
Output: "me see doggy me pet kitty me see birdie"
```

---

## ‚öôÔ∏è Customization Cheat Sheet

### Make it Bigger (Smarter)
```python
baby.build_model(lstm_units=256)  # Default: 64-128
```

### Train Longer (Better)
```python
baby.train(epochs=500)  # Default: 150
```

### More Data (Best!)
```python
BABY_DATASET = [
    "me want cookie",
    # Add 100 more sentences here!
]
```

### Control Creativity
```python
baby.generate_text("me want", temperature=0.5)  # Conservative
baby.generate_text("me want", temperature=1.0)  # Balanced
baby.generate_text("me want", temperature=1.5)  # Creative
baby.generate_text("me want", temperature=2.0)  # Chaos!
```

---

## üéØ Your Next 5 Minutes

```bash
# 1. Install (10 seconds)
pip install tensorflow numpy

# 2. Run (3 minutes training)
python simple_run.py

# 3. Celebrate! üéâ
# You just trained an RNN language model!
```

---

## üìö Learn More

| File | What You'll Learn |
|------|------------------|
| `QUICK_START.md` | How to run everything |
| `ARCHITECTURE_EXPLAINED.md` | How RNN/LSTM works |
| `BABY_MODEL_GUIDE.md` | Complete documentation |
| `run_baby_model.py` | Code examples |

---

## ‚ùì FAQ

**Q: Is this really using RNN?**
A: Yes! LSTM (the model type) is a special type of RNN.

**Q: How do I run it?**
A: `python simple_run.py` or `python baby_language_model.py`

**Q: Do I need GPU?**
A: No! Runs fine on CPU (2-3 minutes).

**Q: Can I customize the dataset?**
A: Yes! Edit `BABY_DATASET` in any .py file.

**Q: Why does it generate gibberish?**
A: Train longer (more epochs) or add more data.

**Q: What's the difference between LSTM and Transformer?**
A: LSTM is simpler and better for small datasets. Use it!

---

## üéâ Ready to Start?

### The Absolute Easiest Way:
```bash
python simple_run.py
```

### Want More Control?
```bash
python run_baby_model.py
```

### Want to Understand Everything?
Read `ARCHITECTURE_EXPLAINED.md` then run!

---

**You're all set! Let's make that baby talk! üë∂ü§ñ**

```bash
cd /Users/soroushebadi/Desktop/Projects/ML_learning
python simple_run.py
```
