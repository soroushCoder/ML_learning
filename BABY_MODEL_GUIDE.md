# Baby Language Model - Complete Guide üë∂

Build a tiny language model that talks like a baby! Perfect for learning and fun experiments on Google Colab.

## üéØ What You'll Get

A small AI model that generates baby-style text like:
- "me want cookie"
- "me love mommy"
- "me play with toy"

## üìã Table of Contents

1. [Quick Start](#quick-start)
2. [Option 1: LSTM Model (Simpler, Recommended)](#option-1-lstm-model)
3. [Option 2: Transformer Model (More Advanced)](#option-2-transformer-model)
4. [Customizing Your Dataset](#customizing-your-dataset)
5. [Troubleshooting](#troubleshooting)

---

## üöÄ Quick Start

### For Google Colab

1. **Open Google Colab**: Go to https://colab.research.google.com
2. **Create a new notebook**
3. **Upload one of the Python files** (`baby_language_model.py` or `baby_transformer_model.py`)
4. **Install dependencies** (first cell):

```python
# For LSTM model
!pip install tensorflow numpy

# OR for Transformer model
!pip install transformers torch
```

5. **Run the model** (second cell):

```python
# For LSTM model
!python baby_language_model.py

# OR for Transformer model
!python baby_transformer_model.py
```

That's it! The model will train and start generating baby talk.

---

## üß† Option 1: LSTM Model (Simpler, Recommended)

**File**: `baby_language_model.py`

### Why Choose LSTM?
- ‚úÖ Easier to understand
- ‚úÖ Faster training (2-3 minutes)
- ‚úÖ Works great with tiny datasets
- ‚úÖ Less memory usage
- ‚úÖ Better for learning

### Step-by-Step Usage

#### Step 1: Install Dependencies

```bash
pip install tensorflow numpy
```

#### Step 2: Run the Training

```python
from baby_language_model import BabyLanguageModel, BABY_DATASET

# Create model
baby_model = BabyLanguageModel(BABY_DATASET, seq_length=15)

# Build the neural network
baby_model.build_model(lstm_units=64, dropout=0.2)

# Train it!
baby_model.train(epochs=200, batch_size=32)
```

#### Step 3: Generate Baby Talk

```python
# Generate with different creativity levels
baby_model.generate_text("me want", length=40, temperature=1.0)
# Output: "me want cookie me love mommy me play..."

# More conservative (temperature=0.5)
baby_model.generate_text("me want", length=40, temperature=0.5)

# More creative/random (temperature=1.5)
baby_model.generate_text("me want", length=40, temperature=1.5)
```

#### Step 4: Save and Load

```python
# Save your trained model
baby_model.save_model('my_baby_model.keras')

# Later, load it back
baby_model.load_model('my_baby_model.keras')
```

### Complete Example

```python
"""
Complete LSTM Baby Model Example
Run this in Google Colab or locally
"""

from baby_language_model import BabyLanguageModel, BABY_DATASET

# 1. Create and train
baby = BabyLanguageModel(BABY_DATASET)
baby.build_model(lstm_units=64)
baby.train(epochs=200)

# 2. Generate baby talk
prompts = ["me want", "me love", "mommy", "daddy play"]

for prompt in prompts:
    print(f"\nPrompt: {prompt}")
    text = baby.generate_text(prompt, length=50, temperature=1.0)
    print(f"Generated: {text}")

# 3. Save for later
baby.save_model('baby_talk.keras')
```

### Parameters Explained

| Parameter | What it does | Recommended Value |
|-----------|-------------|-------------------|
| `seq_length` | How many characters the model looks at | 15-20 |
| `lstm_units` | Model size (bigger = more complex) | 64-128 |
| `epochs` | Training iterations | 100-200 |
| `temperature` | Creativity (higher = more random) | 0.5-1.5 |

---

## ü§ñ Option 2: Transformer Model (More Advanced)

**File**: `baby_transformer_model.py`

### Why Choose Transformer?
- ‚úÖ State-of-the-art architecture (like GPT)
- ‚úÖ Better for longer text generation
- ‚úÖ Uses Hugging Face (industry standard)
- ‚ö†Ô∏è Slower training (5-10 minutes)
- ‚ö†Ô∏è More complex code

### Step-by-Step Usage

#### Step 1: Install Dependencies

```bash
pip install transformers torch
```

#### Step 2: Run the Training

```python
from baby_transformer_model import BabyTransformer, BABY_DATASET

# Create tiny transformer
baby = BabyTransformer(BABY_DATASET)

# Train (this takes longer than LSTM)
baby.train(epochs=100, batch_size=4, learning_rate=5e-4)
```

#### Step 3: Generate Baby Talk

```python
# Generate one sentence
outputs = baby.generate(
    prompt="me want",
    max_length=20,
    temperature=1.0,
    num_return_sequences=1
)
print(outputs[0])

# Generate multiple variations
outputs = baby.generate(
    prompt="me love",
    max_length=25,
    temperature=1.0,
    num_return_sequences=5  # Get 5 different outputs
)

for i, text in enumerate(outputs):
    print(f"{i+1}. {text}")
```

#### Step 4: Save and Load

```python
# Save
baby.save_model('baby_transformer_model')

# Load
baby.load_model('baby_transformer_model')
```

### Complete Example

```python
"""
Complete Transformer Baby Model Example
"""

from baby_transformer_model import BabyTransformer, BABY_DATASET

# 1. Create and train
baby = BabyTransformer(BABY_DATASET)
baby.train(epochs=100, batch_size=4)

# 2. Generate with different temperatures
for temp in [0.7, 1.0, 1.3]:
    print(f"\nTemperature: {temp}")
    outputs = baby.generate(
        prompt="me want",
        max_length=20,
        temperature=temp,
        num_return_sequences=3
    )
    for text in outputs:
        print(f"  - {text}")

# 3. Save
baby.save_model()
```

---

## üìù Customizing Your Dataset

### Adding Your Own Baby Talk

Edit the `BABY_DATASET` list in either file:

```python
BABY_DATASET = [
    "me want cookie",
    "me love puppy",
    # Add your own sentences here!
    "me ride tricycle",
    "me color with crayons",
    "me watch cartoons",
    # Add as many as you want (minimum ~20 recommended)
]
```

### Tips for Better Results

1. **Size Matters**:
   - Minimum: ~20 sentences
   - Good: ~50 sentences
   - Better: ~100+ sentences

2. **Consistency**:
   - Keep the style consistent
   - Use similar grammar patterns
   - Stick to baby vocabulary

3. **Variety**:
   - Mix different verbs (want, love, see, play, eat)
   - Include different subjects (me, mommy, daddy)
   - Vary sentence length

4. **Examples of Good Additions**:
   ```python
   "me throw ball",
   "me catch ball",
   "me kick ball",
   "daddy throw ball to me",
   "me play catch with daddy",
   ```

### External Dataset

You can also load from a text file:

```python
# Save your dataset to a file
with open('baby_talk.txt', 'w') as f:
    f.write("me want cookie\n")
    f.write("me love toy\n")
    # ... more lines

# Load it
with open('baby_talk.txt', 'r') as f:
    BABY_DATASET = [line.strip() for line in f.readlines()]
```

---

## üéÆ Interactive Examples

### Example 1: Conversation Simulator

```python
"""Generate a baby conversation"""

prompts = [
    "mommy",
    "me hungry",
    "me want",
    "thank you",
    "me love"
]

for prompt in prompts:
    response = baby_model.generate_text(prompt, length=30, temperature=0.8)
    print(f"üë∂: {response}\n")
```

### Example 2: Temperature Comparison

```python
"""See how temperature affects output"""

prompt = "me want"

print("Conservative (temp=0.5) - More predictable:")
print(baby_model.generate_text(prompt, length=50, temperature=0.5))

print("\nBalanced (temp=1.0) - Normal:")
print(baby_model.generate_text(prompt, length=50, temperature=1.0))

print("\nCreative (temp=1.5) - More random:")
print(baby_model.generate_text(prompt, length=50, temperature=1.5))
```

### Example 3: Story Generator

```python
"""Generate a baby story"""

story_prompts = [
    "me wake up",
    "me hungry",
    "me eat",
    "me play",
    "me tired",
    "me sleep"
]

print("Baby's Day:\n")
for prompt in story_prompts:
    sentence = baby_model.generate_text(prompt, length=25, temperature=0.8)
    # Get just the first sentence
    sentence = sentence.split('\n')[0]
    print(f"üìñ {sentence}")
```

---

## üîß Troubleshooting

### Common Issues

#### 1. "No module named 'tensorflow'"

**Solution**:
```bash
pip install tensorflow
# or for Apple Silicon Macs:
pip install tensorflow-macos tensorflow-metal
```

#### 2. "No module named 'transformers'"

**Solution**:
```bash
pip install transformers torch
```

#### 3. "Model generates gibberish"

**Causes & Solutions**:
- **Too few training examples**: Add more sentences (aim for 50+)
- **Need more training**: Increase epochs (try 300-500)
- **Temperature too high**: Lower to 0.5-0.8
- **Sequence too short**: Increase `seq_length` to 20-25

#### 4. "Training is too slow"

**Solutions**:
- Use LSTM model instead of Transformer
- Reduce `epochs` to 50-100
- Reduce `lstm_units` to 32
- Use Google Colab with GPU:
  ```
  Runtime ‚Üí Change runtime type ‚Üí GPU
  ```

#### 5. "Generated text is too repetitive"

**Solutions**:
- Increase `temperature` (try 1.2-1.5)
- Add more variety to your dataset
- Train for more epochs

#### 6. "Out of memory error"

**Solutions**:
- Reduce `batch_size` (try 16 or 8)
- Reduce `lstm_units` (try 32)
- Use smaller `seq_length` (try 10)

---

## üìä Understanding Training Output

### LSTM Model Output

```
Epoch 1/200
32/32 [==============================] - 2s 45ms/step - loss: 2.8453 - accuracy: 0.2341
```

- **loss**: Lower is better (aim for <0.5)
- **accuracy**: Higher is better (aim for >0.8)

### Transformer Model Output

```
Step 100: loss=1.234
```

- Watch loss decrease over time
- Should go from ~3.0 down to ~0.5-1.0

---

## üéì Learning Resources

### Understanding the Code

#### LSTM Model (`baby_language_model.py`)
- **Character-level**: Predicts one character at a time
- **LSTM layers**: Remember previous characters
- **Embedding**: Converts characters to numbers
- **Softmax**: Chooses next character

#### Transformer Model (`baby_transformer_model.py`)
- **Token-level**: Works with word pieces
- **Self-attention**: Looks at all previous words
- **GPT-style**: Same architecture as ChatGPT (but tiny!)

### Key Concepts

1. **Temperature**: Controls randomness
   - Low (0.5): Safe, predictable
   - Medium (1.0): Balanced
   - High (1.5): Creative, risky

2. **Epochs**: Training passes through data
   - More epochs = better learning
   - Too many = overfitting (memorization)

3. **Sequence Length**: Context window
   - Longer = better context
   - Longer = more memory/slower

---

## üöÄ Next Steps

### Make It Better

1. **Collect more data**: Add 100-200 more sentences
2. **Fine-tune parameters**: Experiment with different values
3. **Add context**: Include scenarios (bedtime, mealtime, playtime)
4. **Train longer**: Try 500-1000 epochs
5. **Use GPU**: Speed up training 10-100x

### Fun Experiments

1. **Different characters**:
   - Pirate talk: "arrr me want treasure"
   - Robot talk: "UNIT-01 requires energy"
   - Caveman talk: "me hunt mammoth"

2. **Dialogue system**:
   - Train on baby + parent conversations
   - Generate back-and-forth exchanges

3. **Multi-age model**:
   - Train separate models for ages 1-2, 2-3, 3-4
   - See language progression

---

## üìö Additional Resources

- **TensorFlow Tutorial**: https://www.tensorflow.org/tutorials
- **Hugging Face Docs**: https://huggingface.co/docs
- **Understanding LSTMs**: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
- **GPT Explained**: https://jalammar.github.io/illustrated-gpt2/

---

## ‚ùì FAQ

**Q: Which model should I use?**
A: Start with LSTM (`baby_language_model.py`) - it's simpler and faster.

**Q: How much data do I need?**
A: Minimum 20 sentences, but 50-100 is better.

**Q: Can this run on my laptop?**
A: Yes! Both models are tiny and CPU-friendly.

**Q: How long does training take?**
A: LSTM: 2-3 minutes, Transformer: 5-10 minutes (on CPU).

**Q: Can I use this for other languages?**
A: Yes! Just change the dataset to your language.

**Q: The output doesn't make sense!**
A: Try: (1) more data, (2) more epochs, (3) lower temperature.

---

## üéâ Have Fun!

You now have everything you need to build a baby-talking AI! Start experimenting and have fun! üë∂ü§ñ

If you make something cool, share it!

---

**Made with ‚ù§Ô∏è for learning and fun**
